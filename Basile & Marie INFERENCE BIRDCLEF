{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"},{"sourceId":172963793,"sourceType":"kernelVersion"},{"sourceId":6125,"sourceType":"modelInstanceVersion","modelInstanceId":4596},{"sourceId":6127,"sourceType":"modelInstanceVersion","modelInstanceId":4598}],"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# # PROJET BIRDCLEF Marie & Basile\n\n# %% [markdown]\n# BirdCLEF 2024 avec KerasCV et Keras\n# L'objectif de cette compétition est d'identifier les espèces d'oiseaux indiens peu étudiées par leurs appels.\n# \n# Ce notebook, réalisé par Marie et Basile, vous guide à travers le processus d'inférence d'un modèle de Deep Learning pour reconnaître les espèces d'oiseaux par leurs chants (données audio). Étant donné que l'inférence nécessite de fonctionner uniquement sur le CPU, nous avons dû créer des notebooks séparés pour l'entraînement et l'inférence. Vous pouvez trouver le notebook d'entraînement ici. Pour rappel, le notebook d'entraînement utilise le backbone EfficientNetV2 de KerasCV sur le dataset de la compétition. Ce notebook montre également comment convertir les données audio en spectrogrammes Mel à l'aide de Keras.\n# \n# <u>Fait amusant</u> : Les notebooks d'entraînement et d'inférence sont agnostiques par rapport au backend, prenant en charge TensorFlow, PyTorch et JAX. Utiliser KerasCV et Keras nous permet de choisir notre backend préféré. Explorez plus de détails sur Keras.\n# \n# Dans ce notebook, vous apprendrez :\n# \n# À concevoir un pipeline de données pour les données audio, y compris la conversion audio en spectrogrammes.\n# À charger les données efficacement en utilisant tf.data.\n# À créer le modèle en utilisant les presets de KerasCV.\n# À inférer le modèle entraîné.\n# Note : Pour une compréhension plus approfondie de KerasCV, référez-vous aux guides KerasCV.\n\n# %% [markdown] {\"papermill\":{\"duration\":0.065343,\"end_time\":\"2022-03-08T03:18:11.885586\",\"exception\":false,\"start_time\":\"2022-03-08T03:18:11.820243\",\"status\":\"completed\"},\"tags\":[]}\n# # Import Libraries \n\n# %% [code] {\"papermill\":{\"duration\":2.632068,\"end_time\":\"2022-03-08T03:18:14.585094\",\"exception\":false,\"start_time\":\"2022-03-08T03:18:11.953026\",\"status\":\"completed\"},\"tags\":[],\"_kg_hide-output\":true,\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:37:47.471736Z\",\"iopub.execute_input\":\"2024-06-06T11:37:47.472110Z\",\"iopub.status.idle\":\"2024-06-06T11:38:17.523324Z\",\"shell.execute_reply.started\":\"2024-06-06T11:37:47.472069Z\",\"shell.execute_reply\":\"2024-06-06T11:38:17.522329Z\"}}\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # \"jax\" or \"tensorflow\" or \"torch\" \n\nimport keras_cv\nimport keras\nimport keras.backend as K\nimport tensorflow as tf\nimport tensorflow_io as tfio\n\nimport numpy as np \nimport pandas as pd\n\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport librosa\nimport IPython.display as ipd\nimport librosa.display as lid\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\ncmap = mpl.cm.get_cmap('coolwarm')\n\n# %% [markdown] {\"papermill\":{\"duration\":0.066353,\"end_time\":\"2022-03-08T03:18:18.099835\",\"exception\":false,\"start_time\":\"2022-03-08T03:18:18.033482\",\"status\":\"completed\"},\"tags\":[]}\n# # Configuration\n\n# %% [code] {\"papermill\":{\"duration\":0.156464,\"end_time\":\"2022-03-08T03:18:18.322809\",\"exception\":false,\"start_time\":\"2022-03-08T03:18:18.166345\",\"status\":\"completed\"},\"tags\":[],\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:17.525257Z\",\"iopub.execute_input\":\"2024-06-06T11:38:17.525772Z\",\"iopub.status.idle\":\"2024-06-06T11:38:17.546224Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:17.525745Z\",\"shell.execute_reply\":\"2024-06-06T11:38:17.545287Z\"}}\nclass CFG:\n    seed = 42\n    \n    # Input image size and batch size\n    img_size = [128, 384]\n    \n    # Audio duration, sample rate, and length\n    duration = 15 # second\n    sample_rate = 32000\n    audio_len = duration*sample_rate\n    \n    # STFT parameters\n    nfft = 2028\n    window = 2048\n    hop_length = audio_len // (img_size[1] - 1)\n    fmin = 20\n    fmax = 16000\n    \n    # Number of epochs, model name\n    preset = 'efficientnetv2_b2_imagenet'\n\n    # Class Labels for BirdCLEF 24\n    class_names = sorted(os.listdir('/kaggle/input/birdclef-2024/train_audio/'))\n    num_classes = len(class_names)\n    class_labels = list(range(num_classes))\n    label2name = dict(zip(class_labels, class_names))\n    name2label = {v:k for k,v in label2name.items()}\n\n# %% [markdown] {\"papermill\":{\"duration\":0.070351,\"end_time\":\"2022-03-08T03:18:18.46058\",\"exception\":false,\"start_time\":\"2022-03-08T03:18:18.390229\",\"status\":\"completed\"},\"tags\":[]}\n# # Reproducibility\n\n# %% [code] {\"papermill\":{\"duration\":0.153451,\"end_time\":\"2022-03-08T03:18:18.685056\",\"exception\":false,\"start_time\":\"2022-03-08T03:18:18.531605\",\"status\":\"completed\"},\"tags\":[],\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:17.547292Z\",\"iopub.execute_input\":\"2024-06-06T11:38:17.547589Z\",\"iopub.status.idle\":\"2024-06-06T11:38:17.554194Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:17.547556Z\",\"shell.execute_reply\":\"2024-06-06T11:38:17.553385Z\"}}\ntf.keras.utils.set_random_seed(CFG.seed)\n\n# %% [markdown]\n# # Dataset Path\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:17.555286Z\",\"iopub.execute_input\":\"2024-06-06T11:38:17.555872Z\",\"iopub.status.idle\":\"2024-06-06T11:38:17.567153Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:17.555840Z\",\"shell.execute_reply\":\"2024-06-06T11:38:17.566452Z\"}}\nBASE_PATH = '/kaggle/input/birdclef-2024'\n\n# %% [markdown] {\"papermill\":{\"duration\":0.067107,\"end_time\":\"2022-03-08T03:18:26.962626\",\"exception\":false,\"start_time\":\"2022-03-08T03:18:26.895519\",\"status\":\"completed\"},\"tags\":[]}\n# # Test Data \n\n# %% [code] {\"papermill\":{\"duration\":0.241649,\"end_time\":\"2022-03-08T03:18:27.408813\",\"exception\":false,\"start_time\":\"2022-03-08T03:18:27.167164\",\"status\":\"completed\"},\"tags\":[],\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:17.569714Z\",\"iopub.execute_input\":\"2024-06-06T11:38:17.569964Z\",\"iopub.status.idle\":\"2024-06-06T11:38:17.753641Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:17.569944Z\",\"shell.execute_reply\":\"2024-06-06T11:38:17.752816Z\"}}\ntest_paths = glob(f'{BASE_PATH}/test_soundscapes/*ogg')\n# During commit use `unlabeled` data as there is no `test` data.\n# During submission `test` data will automatically be populated.\nif len(test_paths)==0:\n    test_paths = glob(f'{BASE_PATH}/unlabeled_soundscapes/*ogg')[:10]\ntest_df = pd.DataFrame(test_paths, columns=['filepath'])\ntest_df.head()\n\n# %% [markdown] {\"papermill\":{\"duration\":0.182769,\"end_time\":\"2022-03-08T03:20:04.861966\",\"exception\":false,\"start_time\":\"2022-03-08T03:20:04.679197\",\"status\":\"completed\"},\"tags\":[]}\n# # Modeling\n\n# %% [code] {\"papermill\":{\"duration\":1.239321,\"end_time\":\"2022-03-08T03:20:06.281118\",\"exception\":false,\"start_time\":\"2022-03-08T03:20:05.041797\",\"status\":\"completed\"},\"tags\":[],\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:17.754731Z\",\"iopub.execute_input\":\"2024-06-06T11:38:17.755057Z\",\"iopub.status.idle\":\"2024-06-06T11:38:25.785413Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:17.755012Z\",\"shell.execute_reply\":\"2024-06-06T11:38:25.784425Z\"}}\n# Create an input layer for the model\ninp = keras.layers.Input(shape=(None, None, 3))\n# Pretrained backbone\nbackbone = keras_cv.models.EfficientNetV2Backbone.from_preset(\n    CFG.preset,\n)\nout = keras_cv.models.ImageClassifier(\n    backbone=backbone,\n    num_classes=CFG.num_classes,\n    name=\"classifier\"\n)(inp)\n# Build model\nmodel = keras.models.Model(inputs=inp, outputs=out)\n# Load weights of trained model\nmodel.load_weights(\"/kaggle/input/birdclef24-kerascv-starter-train/best_model.weights.h5\")\n\n# %% [markdown]\n# # Data Loader \n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:25.787125Z\",\"iopub.execute_input\":\"2024-06-06T11:38:25.787792Z\",\"iopub.status.idle\":\"2024-06-06T11:38:25.799782Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:25.787757Z\",\"shell.execute_reply\":\"2024-06-06T11:38:25.798953Z\"}}\n# Decodes Audio\ndef build_decoder(with_labels=True, dim=1024):\n    def get_audio(filepath):\n        file_bytes = tf.io.read_file(filepath)\n        audio = tfio.audio.decode_vorbis(file_bytes) # decode .ogg file\n        audio = tf.cast(audio, tf.float32)\n        if tf.shape(audio)[1]>1: # stereo -> mono\n            audio = audio[...,0:1]\n        audio = tf.squeeze(audio, axis=-1)\n        return audio\n    \n    def create_frames(audio, duration=5, sr=32000):\n        frame_size = int(duration * sr)\n        audio = tf.pad(audio[..., None], [[0, tf.shape(audio)[0] % frame_size], [0, 0]]) # pad the end\n        audio = tf.squeeze(audio) # remove extra dimension added for padding\n        frames = tf.reshape(audio, [-1, frame_size]) # shape: [num_frames, frame_size]\n        return frames\n    \n    def apply_preproc(spec):\n        # Standardize\n        mean = tf.math.reduce_mean(spec)\n        std = tf.math.reduce_std(spec)\n        spec = tf.where(tf.math.equal(std, 0), spec - mean, (spec - mean) / std)\n\n        # Normalize using Min-Max\n        min_val = tf.math.reduce_min(spec)\n        max_val = tf.math.reduce_max(spec)\n        spec = tf.where(tf.math.equal(max_val - min_val, 0), spec - min_val,\n                              (spec - min_val) / (max_val - min_val))\n        return spec\n\n    def decode(path):\n        # Load audio file\n        audio = get_audio(path)\n        # Split audio file into frames with each having 5 seecond duration\n        audio = create_frames(audio)\n        # Convert audio to spectrogram\n        spec = keras.layers.MelSpectrogram(num_mel_bins=CFG.img_size[0],\n                                             fft_length=CFG.nfft, \n                                              sequence_stride=CFG.hop_length, \n                                              sampling_rate=CFG.sample_rate)(audio)\n        # Apply normalization and standardization\n        spec = apply_preproc(spec)\n        # Covnert spectrogram to 3 channel image (for imagenet)\n        spec = tf.tile(spec[..., None], [1, 1, 1, 3])\n        return spec\n    \n    return decode\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:25.801017Z\",\"iopub.execute_input\":\"2024-06-06T11:38:25.801361Z\",\"iopub.status.idle\":\"2024-06-06T11:38:25.815643Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:25.801332Z\",\"shell.execute_reply\":\"2024-06-06T11:38:25.814819Z\"}}\n# Build data loader\ndef build_dataset(paths, batch_size=1, decode_fn=None, cache=False):\n    if decode_fn is None:\n        decode_fn = build_decoder(dim=CFG.audio_len) # decoder\n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = (paths,)\n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO) # decode audio to spectrograms then create frames\n    ds = ds.cache() if cache else ds # cache files\n    ds = ds.batch(batch_size, drop_remainder=False) # create batches\n    ds = ds.prefetch(AUTO)\n    return ds\n\n# %% [markdown]\n# # Inference \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:25.816745Z\",\"iopub.execute_input\":\"2024-06-06T11:38:25.816994Z\",\"iopub.status.idle\":\"2024-06-06T11:38:59.119999Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:25.816972Z\",\"shell.execute_reply\":\"2024-06-06T11:38:59.119155Z\"}}\n# Initialize empty list to store ids\nids = []\n\n# Initialize empty array to store predictions\npreds = np.empty(shape=(0, CFG.num_classes), dtype='float32')\n\n# Build test dataset\ntest_paths = test_df.filepath.tolist()\ntest_ds = build_dataset(paths=test_paths, batch_size=1)\n\n# Iterate over each audio file in the test dataset\nfor idx, specs in enumerate(tqdm(iter(test_ds), desc='test ', total=len(test_df))):\n    # Extract the filename without the extension\n    filename = test_paths[idx].split('/')[-1].replace('.ogg','')\n    \n    # Convert to backend-specific tensor while excluding extra dimension\n    specs = keras.ops.convert_to_tensor(specs[0])\n    \n    # Predict bird species for all frames in a recording using all trained models\n    frame_preds = model.predict(specs, verbose=0)\n    \n    # Create a ID for each frame in a recording using the filename and frame number\n    frame_ids = [f'{filename}_{(frame_id+1)*5}' for frame_id in range(len(frame_preds))]\n    \n    # Concatenate the ids\n    ids += frame_ids\n    # Concatenate the predictions\n    preds = np.concatenate([preds, frame_preds], axis=0)\n\n# %% [markdown]\n# # Submission ✉️\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T11:38:59.121631Z\",\"iopub.execute_input\":\"2024-06-06T11:38:59.122090Z\",\"iopub.status.idle\":\"2024-06-06T11:38:59.385055Z\",\"shell.execute_reply.started\":\"2024-06-06T11:38:59.122054Z\",\"shell.execute_reply\":\"2024-06-06T11:38:59.384107Z\"}}\n# Submit prediction\npred_df = pd.DataFrame(ids, columns=['row_id'])\npred_df.loc[:, CFG.class_names] = preds\npred_df.to_csv('submission.csv',index=False)\npred_df.head()","metadata":{"_uuid":"4f8222b7-060e-4909-9168-27a9cac61213","_cell_guid":"1ae1e7ec-6a53-4cee-bdd0-baa967205f01","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}